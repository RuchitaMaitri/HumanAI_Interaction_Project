# Using AI for Accessibility
This project is created by Ruchita Maitri for the course 05-318 Human AI-Interation Course at Carnegie Mellon University.


## This is a preliminary attempt to develop an android application to help people with visual impairment navigate better!

People with visual impairment find it difficult to carry out their daily activities. With time they do get used to their surroundings. However, navigating outside comfortable space remains a challenge. This necessitates a one stop solution which can describe the surrounding for visually impaired people in an effective manner. Live surrounding captioning would be ideal. With this motivation in my mind, I decided to develop an android applicationt to predict the traffic light color in real-time to give output in an audio format. For the purpose of academic project, due to limited time and resources, I have kept the scope very limited. 

The three stages of this project are - 
#### 1. Collecting data:
There are plenty of resources available which capture traffic related image data for developing self-driving cars. For this project I have used super-clean data from Udacity's Intro to Self-Driving Car course project (Link in the resources section). This data has approximately 1500 images of traffic signal lights red, yellow and green. Yellow signal data is very limited, close to only ~100 images. This has definitely affected model's prediction capacity for this class.

#### 2. Model Development:

3. Android Application Development



